{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from time import monotonic\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "from translation_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '../data'\n",
    "assert os.path.isdir(data_root_dir)\n",
    "\n",
    "common_path_prefix = data_root_dir + '/ParallelEnRu/OpenSubtitlesv2018/en-ru-rev-norm'\n",
    "train_path_prefix = common_path_prefix + '-train'\n",
    "dev_path_prefix   = common_path_prefix + '-dev'\n",
    "\n",
    "train_bpe_en_path          = train_path_prefix + '-bpe-40k-en.txt'\n",
    "train_bpe_ru_path          = train_path_prefix + '-bpe-40k-ru.txt'\n",
    "train_vocab_en_path        = train_path_prefix + '-bpe-40k-vocab-en.txt'\n",
    "train_vocab_ru_path        = train_path_prefix + '-bpe-40k-vocab-ru.txt'\n",
    "train_dataset_tok_ids_path = train_path_prefix + '-bpe-40k-tok-ids.txt'\n",
    "\n",
    "dev_dataset_tok_ids_path = dev_path_prefix + '-bpe-40k-tok-ids.txt'\n",
    "dev_dataset_toks_path    = dev_path_prefix + '-bpe-40k-toks.txt'\n",
    "    \n",
    "\n",
    "songs_dataset_path_prefix = data_root_dir + '/ParallelEnRu/Songs' + \\\n",
    "                               '/songs-adj-en-ru-lines-rev-norm'\n",
    "train_songs_dataset_path = songs_dataset_path_prefix + '-train.jsonl'\n",
    "dev_songs_dataset_path = songs_dataset_path_prefix + '-dev.jsonl'\n",
    "\n",
    "\n",
    "classic_russian_poetry_dataset_path = data_root_dir + '/PoetryRussian/PoetryCorpus' + \\\n",
    "                                      '/en-ru-backtrans-rev-norm.jsonl'\n",
    "\n",
    "\n",
    "sonnets_common_path_prefix = data_root_dir + '/Sonnets/ShakespeareSonnets' + \\\n",
    "                             '/shakespeare-sonnets-en-ru-marshak-14-lines-rev-norm'\n",
    "\n",
    "train_sonnets_dataset_path = sonnets_common_path_prefix + '-train.jsonl'\n",
    "dev_sonnets_dataset_path   = sonnets_common_path_prefix + '-dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\n",
    "    src_lang = Lang.EN,\n",
    "    tgt_lang = Lang.RU,\n",
    "    src_lower = True,\n",
    "    tgt_lower = True,\n",
    "    src_reversed = True,\n",
    "    tgt_reversed = True,\n",
    "    src_bpe_path = train_bpe_en_path,\n",
    "    tgt_bpe_path = train_bpe_ru_path,\n",
    "    src_vocab_path = train_vocab_en_path,\n",
    "    tgt_vocab_path = train_vocab_ru_path,\n",
    "    train_dataset_tok_ids_path = train_dataset_tok_ids_path,\n",
    "    dev_dataset_tok_ids_path = dev_dataset_tok_ids_path,\n",
    "    dev_dataset_toks_path = dev_dataset_toks_path,\n",
    "    \n",
    "    additional_datasets = [AdditionalDataset(name = 'songs-train',\n",
    "                                             path = train_songs_dataset_path,\n",
    "                                             reversed_ = True,\n",
    "                                             store_text = False),\n",
    "                           \n",
    "                           AdditionalDataset(name = 'songs-dev',\n",
    "                                             path = dev_songs_dataset_path,\n",
    "                                             reversed_ = True,\n",
    "                                             store_text = True),\n",
    "                           \n",
    "                           AdditionalDataset(name = 'poetry',\n",
    "                                             path = classic_russian_poetry_dataset_path,\n",
    "                                             reversed_ = True,\n",
    "                                             store_text = False),\n",
    "                           \n",
    "                           AdditionalDataset(name = 'sonnets-train',\n",
    "                                             path = train_sonnets_dataset_path,\n",
    "                                             reversed_ = True,\n",
    "                                             store_text = True),\n",
    "                           \n",
    "                           AdditionalDataset(name = 'sonnets-dev',\n",
    "                                             path = dev_sonnets_dataset_path,\n",
    "                                             reversed_ = True,\n",
    "                                             store_text = True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonnet generally consists of 14 lines but some of them more, like 15 in this Shakeapeare sonnet:\n",
    "\n",
    "```\n",
    "The forward violet thus did I chide,\n",
    "Sweet thief, whence didst thou steal thy sweet that smells,\n",
    "If not from my love’s breath? The purple pride\n",
    "Which on thy soft check for complexion dwells,\n",
    "In my love’s veins thou hast too grossly dyed.\n",
    "The lily I condemned for thy hand,\n",
    "And buds of marjoram had stol’n thy hair,\n",
    "The roses fearfully on thorns did stand,\n",
    "One blushing shame, another white despair:\n",
    "A third nor red, nor white, had stol’n of both,\n",
    "And to his robbery had annexed thy breath,\n",
    "But for his theft in pride of all his growth\n",
    "A vengeful canker eat him up to death.\n",
    "More flowers I noted, yet I none could see,\n",
    "But sweet, or colour it had stol’n from thee.```\n",
    "\n",
    "We will drop such sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sonnet_lines_english, train_sonnet_lines_russian = [], []\n",
    "for _, (src_lines, tgt_lines) in dataset.iterate_additional_minibatches('sonnets-train',\n",
    "                                                                        max_batch_matrix_width = None,\n",
    "                                                                        batch_size = 1000,\n",
    "                                                                        epoch_count = 1,\n",
    "                                                                        shuffle = False):\n",
    "    train_sonnet_lines_english += src_lines\n",
    "    train_sonnet_lines_russian += tgt_lines\n",
    "\n",
    "dev_sonnet_lines_english, dev_sonnet_lines_russian = [], []\n",
    "for _, (src_lines, tgt_lines) in dataset.iterate_additional_minibatches('sonnets-dev',\n",
    "                                                                        max_batch_matrix_width = None,\n",
    "                                                                        batch_size = 1000,\n",
    "                                                                        epoch_count = 1,\n",
    "                                                                        shuffle = False):\n",
    "    dev_sonnet_lines_english += src_lines\n",
    "    dev_sonnet_lines_russian += tgt_lines\n",
    "\n",
    "print('Train sonnet line count: {}'.format(len(train_sonnet_lines_english)))\n",
    "print('Dev sonnet line count:   {}'.format(len(dev_sonnet_lines_english)))\n",
    "print()\n",
    "print('Train/dev Russian sonnet line examples:')\n",
    "print(train_sonnet_lines_russian[0])\n",
    "print(dev_sonnet_lines_russian[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, some lines don't correspond to -+-+-+-+-+ pattern but correspond to -+-+-+-+-+-\n",
    "\n",
    "Let us try to filter out non-10 syllables lines and see how much lines will remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables_russian(line):\n",
    "    vowels = 'àå¸èîóûýþÿ'\n",
    "    return sum(c in vowels for c in line)\n",
    "\n",
    "train_sonnet_lines_russian_ten_syllables_count = sum(count_syllables_russian(line) == 10\n",
    "                                                     for line in train_sonnet_lines_russian)\n",
    "\n",
    "dev_sonnet_lines_russian_ten_syllables_count = sum(count_syllables_russian(line) == 10\n",
    "                                                     for line in dev_sonnet_lines_russian)\n",
    "\n",
    "print('Russian sonnet line count with 10 syllables:')\n",
    "\n",
    "print(' train : {} of {} ({:.0f}%)'.format(train_sonnet_lines_russian_ten_syllables_count,\n",
    "                                          len(train_sonnet_lines_english),\n",
    "                                          100 * train_sonnet_lines_russian_ten_syllables_count\n",
    "                                          / len(train_sonnet_lines_english)))\n",
    "\n",
    "print(' dev:    {} of {} ({:.0f}%)'.format(dev_sonnet_lines_russian_ten_syllables_count,\n",
    "                                          len(dev_sonnet_lines_english),\n",
    "                                          100 * dev_sonnet_lines_russian_ten_syllables_count\n",
    "                                          / len(dev_sonnet_lines_english)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too little lines remain if we filter them by syllables count thus let us not filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stresses(line):\n",
    "    return np.array([i % 2 == 1 for i in range(count_syllables_russian(line))])\n",
    "\n",
    "train_sonnet_lines_russian_stresses = list(map(make_stresses, train_sonnet_lines_russian))\n",
    "dev_sonnet_lines_russian_stresses = list(map(make_stresses, dev_sonnet_lines_russian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ru_char_encoder import RuCharEncoder # For config\n",
    "from meter_model import MeterModel # For config\n",
    "from translation_model import TranslationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def compute_batch_bleu(model, batch_data):\n",
    "\n",
    "    (src_matrix, _), (_, tgt_lines) = batch_data\n",
    "    \n",
    "    translations = model.translate_matrix(src_matrix,\n",
    "                                          max_output_token_count = 200)\n",
    "    return corpus_bleu(list_of_references = [(ref,) for ref in tgt_lines],\n",
    "                       hypotheses = translations)\n",
    "\n",
    "def compute_dev_bleu(model, dataset):\n",
    "    \n",
    "    batch_iterator = dataset.iterate_dev_minibatches(max_batch_matrix_width = None,\n",
    "                                                     batch_size = 256,\n",
    "                                                     epoch_count = 1)\n",
    "    \n",
    "    result, data_len = 0, 0\n",
    "    \n",
    "    for batch_data in batch_iterator:\n",
    "        \n",
    "        batch_size = len(batch_data[0][0])\n",
    "        \n",
    "        result += compute_batch_bleu(model, batch_data) * batch_size\n",
    "        data_len += batch_size\n",
    "    \n",
    "    result /= data_len\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_config = TranslationModel.Config(emb_size = 128,\n",
    "                                       hid_size = 256,\n",
    "                                       batch_size = 32,\n",
    "                                       ru_char_encoder_config = RuCharEncoder.deepspeare_en_config,\n",
    "                                       meter_config = MeterModel.deepspeare_config)\n",
    "model_name = 'subtitles_songs_poetry_with_meter_shakespeare'\n",
    "\n",
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = TranslationModel(sess,\n",
    "                         filename = None,\n",
    "                         name = model_name,\n",
    "                         inp_tokenizer = dataset.get_tokenizer(Lang.EN),\n",
    "                         out_tokenizer = dataset.get_tokenizer(Lang.RU),\n",
    "                         config = model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_inp = tf.placeholder('int32', [None, None])\n",
    "translator_out = tf.placeholder('int32', [None, None])\n",
    "translator_out_char = tf.placeholder('int32', [None, None, None])\n",
    "\n",
    "meter_inp = model.meter_model.get_input()\n",
    "meter_out = model.meter_model.get_output()\n",
    "\n",
    "translator_loss = model.compute_loss(translator_inp, translator_out, translator_out_char)\n",
    "translator_train_step = tf.train.AdamOptimizer().minimize(translator_loss)\n",
    "meter_loss = model.meter_model.pm_mean_cost\n",
    "meter_train_step = model.meter_model.pm_train_op\n",
    "K.get_session() # To not reset optimizers\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_last_file_test_time = 0\n",
    "\n",
    "def should_stop_train():\n",
    "    global _last_file_test_time\n",
    "    \n",
    "    t = monotonic()\n",
    "    if t - _last_file_test_time > 2: # seconds\n",
    "        _last_file_test_time = t\n",
    "        return os.path.exists('stop-train')\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(NamedTuple):\n",
    "    name: str\n",
    "    history: list\n",
    "\n",
    "def show_history(metrics: List[Metric]):\n",
    "    \n",
    "    plt.figure(figsize=(12,10))\n",
    "    for i, (_, metric) in enumerate(metrics.items()):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.title(metric.name)\n",
    "        plt.plot(*zip(*metric.history))\n",
    "        plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    get_mean = lambda k: np.mean([p[1] for p in metrics[k].history[-10:]])\n",
    "    print('Mean loss: {:.3f}'.format(get_mean('train_loss')))\n",
    "    if metrics['meter_train_loss'].history:\n",
    "        print('Mean meter loss: {:.3f}'.format(get_mean('meter_train_loss')))\n",
    "    print('Mean dev BLEU: {:.3f}'.format(get_mean('dev_bleu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset text pair counts:\n",
    "#\n",
    "#  21M     1    OpenSubtitlesv2018\n",
    "# 1.7M  ~1/10   English song translations\n",
    "# 433k  ~1/50   Classic Russian poetry\n",
    "# 1.8k  ~1/10k  Shakespeare sonnets\n",
    "\n",
    "class TrainCfg:\n",
    "    \n",
    "    use_open_subtitles = True\n",
    "    \n",
    "    songs_step_each_n = 10    # k = 1\n",
    "    poetry_step_each_n   = 23    # k = 2\n",
    "    meter_step_each_n    = 1_000 # k = 10\n",
    "    \n",
    "    bleu_batch_size = 128\n",
    "    bleu_calc_each_n = 100\n",
    "    \n",
    "    save_model_each_n = 10_000\n",
    "\n",
    "cfg = TrainCfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensub_train_batches = dataset.iterate_train_minibatches(max_batch_matrix_width = None,\n",
    "                                                          batch_size = model.config.batch_size,\n",
    "                                                          epoch_count = None) # Never stop\n",
    "\n",
    "opensub_dev_batches = dataset.iterate_dev_minibatches(max_batch_matrix_width = None,\n",
    "                                                      batch_size = cfg.bleu_batch_size,\n",
    "                                                      epoch_count = None, # Never stop\n",
    "                                                      shuffle = True)\n",
    "\n",
    "songs_train_batches = dataset.iterate_additional_minibatches('songs-train',\n",
    "                                                                max_batch_matrix_width = None,\n",
    "                                                                batch_size = model.config.batch_size,\n",
    "                                                                epoch_count = None, # Never stop\n",
    "                                                                shuffle = True)\n",
    "\n",
    "songs_dev_batches = dataset.iterate_additional_minibatches('songs-dev',\n",
    "                                                              max_batch_matrix_width = None,\n",
    "                                                              batch_size = cfg.bleu_batch_size,\n",
    "                                                              epoch_count = None, # Never stop\n",
    "                                                              shuffle = True)\n",
    "\n",
    "poetry_train_batches = dataset.iterate_additional_minibatches('poetry',\n",
    "                                                              max_batch_matrix_width = None,\n",
    "                                                              batch_size = model.config.batch_size,\n",
    "                                                              epoch_count = None, # Never stop\n",
    "                                                              shuffle = True)\n",
    "\n",
    "meter_train_batches = dataset.iterate_additional_minibatches('sonnets-train',\n",
    "                                                             max_batch_matrix_width = None,\n",
    "                                                             batch_size = model.config.batch_size,\n",
    "                                                             epoch_count = None,  # Never stop\n",
    "                                                             shuffle = True,\n",
    "                                                             pad_last_batch = True,\n",
    "                                                             parallel_data = train_sonnet_lines_russian_stresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = OrderedDict(train_loss = Metric('Train loss', []),\n",
    "                      meter_train_loss = Metric('Meter train loss', []),\n",
    "                      dev_bleu = Metric('Dev BLEU', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(iters):\n",
    "    ''' Train translator and meter '''\n",
    "    \n",
    "    dev_batches = opensub_dev_batches if cfg.use_open_subtitles \\\n",
    "                  else songs_dev_batches\n",
    "    \n",
    "    loss_history = metrics['train_loss'].history\n",
    "    start_iter_idx = len(loss_history)\n",
    "    assert iters >= start_iter_idx\n",
    "    \n",
    "    train_iter_count = iters - start_iter_idx\n",
    "    \n",
    "    iter_idx = start_iter_idx\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    stopped = False\n",
    "    \n",
    "    for _ in range(train_iter_count):\n",
    "        \n",
    "        step = iter_idx + 1\n",
    "        print('Step: {}'.format(step), end = '\\r')\n",
    "        \n",
    "        meter_batch = step % cfg.meter_step_each_n == 0\n",
    "        poetry_batch = not meter_batch and step % cfg.poetry_step_each_n == 0\n",
    "        if cfg.use_open_subtitles:\n",
    "            songs_batch = not meter_batch and not poetry_batch \\\n",
    "                             and step % cfg.songs_step_each_n == 0\n",
    "        \n",
    "        if meter_batch:\n",
    "            (src_matrix, tgt_matrix), (_, tgt_lines), stresses = next(meter_train_batches)\n",
    "        elif poetry_batch:\n",
    "            src_matrix, tgt_matrix = next(poetry_train_batches)\n",
    "        elif not cfg.use_open_subtitles or songs_batch:\n",
    "            src_matrix, tgt_matrix = next(songs_train_batches)\n",
    "        else:\n",
    "            src_matrix, tgt_matrix = next(opensub_train_batches)\n",
    "        \n",
    "        # Translator train step\n",
    "        feed_dict = {\n",
    "            translator_inp: src_matrix,\n",
    "            translator_out: tgt_matrix,\n",
    "            translator_out_char: model.to_out_char_matrix(tgt_matrix),\n",
    "        }\n",
    "\n",
    "        translator_loss_t, _ = sess.run([translator_loss, translator_train_step], feed_dict)\n",
    "        loss_history.append((step, translator_loss_t))\n",
    "        \n",
    "        # Meter train step\n",
    "        if meter_batch:\n",
    "\n",
    "            feed_dict = {\n",
    "                meter_inp: model.to_meter_inp_char_matrix(tgt_lines),\n",
    "                meter_out: model.to_meter_out_stress_matrix(stresses)\n",
    "            }\n",
    "\n",
    "            meter_loss_t, _ = sess.run([meter_loss, meter_train_step], feed_dict)\n",
    "            metrics['meter_train_loss'].history.append((step, meter_loss_t))\n",
    "        \n",
    "\n",
    "        if step % cfg.bleu_calc_each_n == 0:\n",
    "            metrics['dev_bleu'].history.append((step, compute_batch_bleu(model, next(dev_batches)) * 100))\n",
    "            clear_output(True)\n",
    "            show_history(metrics)\n",
    "        \n",
    "        if step % cfg.save_model_each_n == 0:\n",
    "            model.dump('{}.pkl'.format(model.name))\n",
    "        \n",
    "        iter_idx += 1\n",
    "        \n",
    "        if should_stop_train():\n",
    "            print('Stopping train.')\n",
    "            stopped = True\n",
    "            break\n",
    "            \n",
    "    train_time = datetime.now() - start_time\n",
    "    \n",
    "    assert len(loss_history) == iter_idx\n",
    "    if not stopped:\n",
    "        assert iter_idx == iters\n",
    "    \n",
    "    if stopped:\n",
    "        print('WARNING: Stopped. Info below')\n",
    "    \n",
    "    print('Model trained for {}{} iters in {}'.format(iter_idx - start_iter_idx,\n",
    "                                                      ' more' if start_iter_idx != 0 else '',\n",
    "                                                      train_time))\n",
    "    \n",
    "    if start_iter_idx != 0:\n",
    "        print('In total model trained for {} iters.'.format(iter_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(iters = 200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(iters = 500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset text pair counts:\n",
    "#\n",
    "# 1.7M     1   English song translations\n",
    "# 433k  ~1/4   Classic Russian poetry\n",
    "# 1.8k  ~1/1k  Shakespeare sonnets\n",
    "\n",
    "cfg.use_open_subtitles = False\n",
    "\n",
    "del cfg.songs_step_each_n\n",
    "cfg.poetry_step_each_n   = 2    # k = 2\n",
    "cfg.meter_step_each_n    = 50   # k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(iters = 550_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dump('{}_500k_iters_50k_fine_tune.pkl'.format(model.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(iters = 600_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dump('{}_500k_iters_100k_fine_tune.pkl'.format(model.name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
